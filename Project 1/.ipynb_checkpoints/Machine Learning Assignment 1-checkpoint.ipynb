{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 461 Machine Learning Assignment 1\n",
    "### 05 Dec. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amer Nour Eddin | 213171245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all cells until you reach the main function pass you required (datapath, test_ration, labelcolumn) to it\n",
    "and it will give you the prepared data. At the end you can evaluate your model and predict values for your label column using \n",
    "predict function.\n",
    "I added tow error mesrueing metrics at the very end of the code as an extra to see how is your model performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PS. I tested my code on 2 other datasets and it performed well given the assignment requirments. (It can be enhanced, but I didn't want to go out the scope of the assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Analysis & Pre-processing part, you are going to implement some functions using the scikit-learn library. You will\n",
    "analyze your dataset and change some values in it in order to prepare your data to be used in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) \n",
    "The create_df function should take the path of the csv file and turn it into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(a)\n",
    "\n",
    "def create_df(data_path):    \n",
    "    # input: the path of the csv file\n",
    "    # output: data frame\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = os.path.realpath(\"housing.csv\")\n",
    "# df = create_df(path)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) \n",
    "The nan_columns function should take a data frame as an input and return a list of names of columns that contain\n",
    "nan values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(b)\n",
    "\n",
    "def nan_columns(df):\n",
    "    # input: data frame\n",
    "    # output: a list of names of columns that contain nan values in the data frame\n",
    "    nancolumns = df.columns[df.isnull().any()].tolist() # take df and list the columns names in which there are NAN values in it\n",
    "\n",
    "    return nancolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nancolumns = nan_columns(df)\n",
    "# nancolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) \n",
    "The categorical_columns function should take a data frame as an input and return a list of column names that contain\n",
    "categorical values in it. \n",
    "For example, if a data frame has ‘gender’ column and ‘female’ and ‘male’ values in it, then ‘gender’ column should\n",
    "be in the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(c)\n",
    "\n",
    "def categorical_columns(df):\n",
    "    # input: data frame\n",
    "    # output: a list of column names that contain categorical values in the data frame\n",
    "    all_c = df.columns # all columns\n",
    "    num_cols = df._get_numeric_data().columns # only numerical columns\n",
    "    catcolumns = list(set(all_c) - set(num_cols)) # subtract all columns from numerical columns => categorical columns(non-numeric)\n",
    "    return catcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# catcolumns = categorical_columns(df)\n",
    "# catcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) \n",
    "The replace_missing_features function should take a data frame and a list of column names that contain nan values\n",
    "as input.The function should replace all nan values in the column with the median of this column’s values and return this\n",
    "new data frame as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(d)\n",
    "\n",
    "def replace_missing_features(df, nancolumns):\n",
    "    # input: data frame, list of column names that contain nan values\n",
    "    # output: data frame\n",
    "    if len(nancolumns) == 0:\n",
    "        new_df1 = df\n",
    "    else:\n",
    "        for c in nancolumns:\n",
    "            new_df1 = df.fillna((df[c]).median()) # fill all NAN elements with the median of the column elements\n",
    "    return new_df1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nancolumns = nan_columns(df)\n",
    "# new_df1 = replace_missing_features(df, nancolumns)\n",
    "# # new_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) \n",
    "The cat_to_num function should take a data frame and a list of categorical feature column names as input. Perform\n",
    "one-hot-encoding on categorical features. Modify your data frame and replace its nominal features with their one-hot\n",
    "encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I WILL USE THIS CATAGORICAL ENCODER IN PART (e) cat_to_num function so you have to compile it first!\n",
    "\n",
    "\n",
    "\n",
    "# # Definition of the CategoricalEncoder class, copied from PR #9151. # #\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(e)\n",
    "\n",
    "def cat_to_num(new_df1, catcolumns):\n",
    "    # input: data frame, list of categorical feature column names\n",
    "    # output: data frame\n",
    "    if len(catcolumns) == 0:\n",
    "        new_df2 = new_df1\n",
    "    else:\n",
    "        housing_cat = new_df1[catcolumns] # take all catagorical columns\n",
    "        cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\") # specify the encoder\n",
    "        housing_cat_reshaped = housing_cat.values.reshape(-1, 1) # reshap\n",
    "        dense_m = cat_encoder.fit_transform(housing_cat_reshaped) # encode 1hot\n",
    "        dense_m_df = pd.DataFrame(dense_m) # convert to dataframe\n",
    "        c = cat_encoder.categories_ # list of the catagories names\n",
    "        dense_m_df.columns = c # assign the catagories names to the encoded dataframe\n",
    "        # add the ready categorical encoded dataframe to the original dataframe and remove the columns that we encoded (non-numeric) \n",
    "        new_df2 = pd.concat([new_df1, dense_m_df], axis=1).drop(catcolumns, 1) \n",
    "    return new_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_df2 = cat_to_num(new_df1, catcolumns )\n",
    "# new_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) \n",
    "The standardization function should take a data frame and label column as input and scale all columns except the\n",
    "label column with standardization. Return a new data frame with scaled values. (10 points)\n",
    "Scikit-Learn provides a transformer called StandardScaler for standardization. The output of the scaler is an\n",
    "array, you need to convert it dataframe after standardization. Don’t forget to add indexes and columns of the\n",
    "original dataframe to the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(f)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "\n",
    "def standardization(new_df2, labelcol):\n",
    "    # input: data frame and name of the label column\n",
    "    # output: scaled data frame\n",
    "    x =  list(new_df2.columns) # x is a list of all column names in the dataframe\n",
    "    x.remove(labelcol) # remove label column from x\n",
    "    x\n",
    "    # set the parameters and pipeline \n",
    "    pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(x)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ]) \n",
    "    new_df3 = pd.DataFrame(pipeline.fit_transform(new_df2)) # fit and transform and convert the standardized data to a new dataframe \n",
    "    new_df3.columns = x # assigne column names to the new dataframe\n",
    "    new_df3 = pd.concat([new_df3, new_df2[labelcol]], axis=1) # add the non-standardized label column to the standardized dataframe\n",
    "    return new_df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_df3 = standardization(new_df2,'price')\n",
    "# new_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g)\n",
    "The my_train_test_split function should take a data frame, name of the label column and percentage of test size as\n",
    "input. Split dataframe as X and y where y is the label column values and X is the feature values (all column values\n",
    "except the label column). Then, the function should split X and y into test and train sets as X_train, X_test, y_train and\n",
    "y_test with the given test size. Output datatype should be numpy array.\n",
    "For example, if the test size is 0.3, then the function should divide the data frame into 70% training and 30% test\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(g)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def my_train_test_split(new_df3, labelcol, test_ratio):\n",
    "    # input: data frame, name of the label column and test data percentage\n",
    "    # output: X_train, X_test, y_train, y_test\n",
    "    np.random.seed(0) # DON'T ERASE THIS LINE\n",
    "    y = new_df3[labelcol].values #t he label column values\n",
    "    X = new_df3.drop(labelcol, 1).values # the feature values (all column values except the label column)\n",
    "    X_train, X_test = train_test_split(X, test_size=test_ratio, random_state=0) # features Train+Test\n",
    "    y_train, y_test = train_test_split(y, test_size=test_ratio, random_state=0) # Labels Train+Test\n",
    "#     X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = my_train_test_split(new_df3, 'price', 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (h)\n",
    "The main function will use all functions you created and return a train and test set at the end. The function should\n",
    "take path of a csv file, test data percentage and name of the label column as an input. First, convert the csv data (which\n",
    "you downloaded at the beginning) into a data frame (a). Fill nan columns of this data frame (b). Find in which column\n",
    "there are categorical values (c). Fill up missing features in data frames (d). Convert categorical features into numerical\n",
    "format (e). Scale all feature columns with standardization (f). Split the final data frame into train and test matrices\n",
    "according to given label column and test ratio. Return X_train, X_test, y_train and y_test matrices (g)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(h)\n",
    "\n",
    "def main(dataPath, testRatio, labelColumn):\n",
    "    # input: the path of the csv file, test data percentage and name of the label column\n",
    "    # output: X_train, X_test, y_train, y_test as numpy arrays\n",
    "    df = create_df(dataPath) #(a)\n",
    "    nancolumns = nan_columns(df) #(b)\n",
    "    catcolumns = categorical_columns(df) #(c)\n",
    "    new_df1 = replace_missing_features(df, nancolumns) #(d)\n",
    "    new_df2 = cat_to_num(new_df1, catcolumns ) #(e)\n",
    "    new_df3 = standardization(new_df2, labelColumn) #(f)    \n",
    "    X_train, X_test, y_train, y_test = my_train_test_split(new_df3, labelColumn, testRatio) #(g)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(\"housing.csv\")\n",
    "X_train, X_test, y_train, y_test = main(path, 0.3, 'median_income') #you can change the parameters for main as you want\n",
    "# X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to train a Linear Regression model that learns to predict the label column according to rest\n",
    "of the features in our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i)\n",
    "model_evaluation function should take training instances (train_x) and labels (train_y) as numpy arrays. Create a\n",
    "LinearRegression model with default parameters. Train the model with the train_x and train_y. Your function should\n",
    "return coef_ and intercept_ arrays of the model as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(i)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def model_evaluation(X_train, y_train):\n",
    "    # input: X_train and y_train matrices\n",
    "    # output: LinearRegression model's coef_ and intercept_ parameters\n",
    "    lin_reg = LinearRegression() # define the Linear Regression model\n",
    "    lin_reg.fit(X_train, y_train) # train it\n",
    "    coef_, intercept_ = [lin_reg.coef_], [lin_reg.intercept_] # get the coeffecient and the intercept values for your defined model\n",
    "    return coef_, intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([  1.53561628e-02,  -3.00066223e-02,  -2.74086746e-01,\n",
       "           1.93936473e+00,  -1.86574215e+00,   5.08633026e-02,\n",
       "          -1.43725983e-01,   1.17283330e+00,   1.65186942e+13,\n",
       "           1.54798631e+13,   5.17559722e+11,   1.04450136e+13,\n",
       "           1.11396031e+13])], [3.8655306274684427])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_, intercept_ = model_evaluation(X_train, y_train)\n",
    "coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (j) \n",
    "predict function should take instance matrix, coef_ array and intercept_ array as input. This function will create a Linear Regression model and set its coef_ and intercept_ parameters to input coef_ and intercept_ values. Using this model, predict the label column values of given instance/s. Return list of predictions as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(j)\n",
    "\n",
    "def predict(instance, coef_, intercept_):\n",
    "    # input: instance matrix, coef_ array and intercept_ array\n",
    "    # ouput: list of predictions for input instances\n",
    "    lin_reg2 = LinearRegression() # define a Linear Regression model\n",
    "    coeff = np.copy(coef_) # coeff holds the coeffecient values in the input\n",
    "    intercept = np.copy(intercept_) # intercept holds the intercept value in the input\n",
    "    lin_reg2.coef_= coeff # set coeff as the coeffecient for your defined model\n",
    "    lin_reg2.intercept_ = intercept # set intercept as the intercept for your defined model\n",
    "    predictions = lin_reg2.predict(instance) # predict\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.32256188],\n",
       "       [ 4.85771813],\n",
       "       [ 4.13115563],\n",
       "       [ 2.29131188],\n",
       "       [ 6.34599938],\n",
       "       [ 2.58037438],\n",
       "       [ 5.10381188],\n",
       "       [ 5.45537438],\n",
       "       [ 6.76787438],\n",
       "       [ 4.40459313]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_test[:10], coef_, intercept_) # Predict first 10 instances from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1518,  5.7796,  4.3487,  2.4511,  5.0049,  2.5682,  6.0661,\n",
       "        5.9275,  6.3373,  3.9167])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10] # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesureing the error value for this Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1547043568210507"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = predict(X_test, coef_, intercept_)\n",
    "lin_mse = mean_squared_error(y_test, predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76544182202657429"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(y_test, predictions)\n",
    "lin_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
