{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 461 Machine Learning Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 Jan. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amer Nour Eddin | 213171245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. We will use the MNIST dataset. First 60000 instances will form your training set, next 10000 instances will be used to create the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "\n",
    "X_train, X_validation_test, y_train, y_validation_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# Training set\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "X_validation_test, y_validation_test = shuffle(X_validation_test, y_validation_test, random_state=0)\n",
    "\n",
    "# Validation set\n",
    "X_validation, y_validation, = X_validation_test[:5000], y_validation_test[:5000]\n",
    "\n",
    "# Test set\n",
    "X_test, y_test = X_validation_test[5000:], y_validation_test[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. \n",
    "Create a Random Forest classifier with parameters random_state=0. Train the classifier using the training set. Save your classifier in pickle format as *RFClassifier.pkl* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RFClassifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rnd_clf, 'RFClassifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "Create an Extra-Trees with parameters random_state=0. Train the classifier using the training set. Save your classifier in pickle format as *ETClassifier.pkl* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(random_state=0)\n",
    "et_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETClassifier.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(et_clf, 'ETClassifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "Combine Random Forest and Extra-Trees classifiers into an ensemble classifier using a soft Voting classifier. Save your trained classifier in pickle format as *SoftEnsembleClassifier.pkl* .\n",
    "\n",
    "You may use the __Scikit-Learn’s VotingClassifier__ .\n",
    "\n",
    "+ Note: In the ensemble classifier, Random Forest and Extra-Trees classifiers are new classifiers,\n",
    "not the classifiers from part a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_..._estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "           verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rnd_clf_ens = RandomForestClassifier(random_state=0)\n",
    "et_clf_ens = ExtraTreesClassifier(random_state=0)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rnd_clf_ens), ('et', et_clf_ens)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftEnsembleClassifier.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(voting_clf, 'SoftEnsembleClassifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "How much better does the ensemble classifier perform compared to the individual classifiers? Use __test set__ to measure __accuracy score__ of each classifier and return them in a single list. Save your result in pickle format as *part_d.pkl* . The order of the classifiers is Random Forest, Extra-Trees and the ensemble classifier. Return your results according to that order.\n",
    "\n",
    "For example, if Random Forest, Extra-Trees and the ensemble classifier has the accuracy score\n",
    "of 0.6, 0.65 and 0.8 respectively, the result will be [0.6, 0.65, 0.8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "result = []\n",
    "for clf in (rnd_clf, et_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    result.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(result, 'part_d.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "Run the individual classifiers (Random Forest and Extra-Trees mentioned in part a and b) to make __probabilistic predictions__ on the __validation set__ and create a new training set with the resulting predictions: each training instance is a vector containing the set of probabilistic predictions from all your classifiers for an image, and the target is the image’s class. Save the\n",
    "new training set into a pickle file as *part_e.pkl* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you are going to predict the image of number 9, firstly, you will predict the probability vector of the image with your individual classifiers’ *predict_proba()* function.\n",
    "\n",
    "Lets say RandomForest classifier’s probabilistic prediction output is:\n",
    "[ 0. 0. 0.1 0. 0.2 0. 0. 0. 0.1 0.6 ]\n",
    "\n",
    "and Extra-Trees classifier’s probabilistic prediction output is:\n",
    "[ 0. 0. 0.1 0.1 0.2 0. 0.2 0. 0. 0.4 ]\n",
    "\n",
    "The new representation of the image will be:\n",
    "[ 0. 0. 0.1 0. 0.2 0. 0. 0. 0.1 0.6 0. 0. 0.1 0.1 0.2 0. 0.2 0. 0. 0.4 ].\n",
    "\n",
    "The new training set will consist of new representation of validation set instances (keep the same order as in the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_propa = rnd_clf.predict_proba(X_validation)\n",
    "rnd_propa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_propa = et_clf.predict_proba(X_validation)\n",
    "et_propa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = []\n",
    "for i in range(len(rnd_propa)):    \n",
    "    x = np.concatenate((rnd_propa[i],et_propa[i]))\n",
    "    z.append(x)\n",
    "\n",
    "new_training =  np.array(z)\n",
    "new_training[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part_e.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_training, 'part_e.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.\n",
    "Train a new Random Forest Classifier (set random_state to 0) with the new training set you created in part e. Save your classifier in pickle format as *Blender.pkl* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have just trained a blender, and together with the classifiers they form a Stacking ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf_blender = RandomForestClassifier(random_state=0)\n",
    "rnd_clf_blender.fit(new_training, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blender.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rnd_clf_blender, 'Blender.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g.\n",
    "Evaluate the ensemble on the test set: for each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. Report the __test accuracy__ of the stacking ensemble in pickle format as *part_g.pkl* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8,  0. ,  0.1,  0.1,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_pred = rnd_clf.predict_proba(X_test)\n",
    "rnd_pred[0]\n",
    "# accuracy_score(y_test, rnd_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7,  0. ,  0.1,  0. ,  0.1,  0. ,  0. ,  0.1,  0. ,  0. ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_pred = et_clf.predict_proba(X_test)\n",
    "et_pred[0]\n",
    "# accuracy_score(y_test, et_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8,  0. ,  0.1,  0.1,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.7,\n",
       "        0. ,  0.1,  0. ,  0.1,  0. ,  0. ,  0.1,  0. ,  0. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = []\n",
    "for i in range(len(rnd_pred)):    \n",
    "    y = np.concatenate((rnd_pred[i],et_pred[i]))\n",
    "    e.append(y)\n",
    "\n",
    "preds =  np.array(e)\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95740000000000003"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Blender_preds = rnd_clf_blender.predict(preds)\n",
    "test_accuracy = accuracy_score(y_test, Blender_preds)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part_g.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test_accuracy, 'part_g.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
